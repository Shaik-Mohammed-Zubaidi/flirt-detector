# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BYjZFkX80YXpJOBJpMvogItfAJR-FLaG
"""
from flask import Flask, request, jsonify
# Import necessary libraries
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

app = Flask(__name__)

# Load the  Flirty ALBERT Model
model = AutoModelForSequenceClassification.from_pretrained("./model_1")
tokenizer = AutoTokenizer.from_pretrained("./model_1/saved_tokenizer")

def flirty_albert_call(msg: str):
    input = tokenizer(msg, return_tensors='pt')

    with torch.no_grad():
        outputs = model(**input)
    logits = outputs.logits

    # Process model outputs
    logits = outputs.logits.squeeze()  # Remove unnecessary dimensions
    probabilities = torch.sigmoid(logits) 

    # Convert probabilities to class labels
    predicted_class = torch.round(probabilities).item()

    if predicted_class == 1:
        rtn_msg = f"Predicted Class: Flirty\n Probability of Being Flirty: {probabilities.item()}"
        return rtn_msg
    else:
        rtn_msg = f"Predicted Class: Not Flirty\n Probability of Being Flirty: {probabilities.item()}"
        return rtn_msg

# Define a route to handle incoming POST requests
@app.route("/api/query", methods=["POST"])
def handle_query():
    try:
        data = request.json
        input_string = data["query"]
        result = flirty_albert_call(input_string)
        # return jsonify({"message": "User data saved successfully."}), 200
        return jsonify({"result": result}), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500
    
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5001)